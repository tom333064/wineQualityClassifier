{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeec7dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Model\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Tuner\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Training\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483e032d",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "### General\n",
    "You can find the Dataset online via: https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009.\n",
    "The Dataset consists of 1143 samples, divided into trainings data and test data. The trainings data contains of 800 samples and the test data contains of 343 samples, the distribution is choosen by the author of this code.\n",
    "### Output data\n",
    "The output data is the \"${\\bf quality}$\" collumn in the dataframe and got 6 different values. Therefore the output data is scaled with an One-Hot-Encoder.\n",
    "### Input data\n",
    "The input data contains 11 features (the \"id\" feature is not used, because it reveals no relevant information about the wine). Some features have a very high mean and therefore are scaled with a standard scaler. The scaled features are: \"${\\bf fixed acidity}$\"(mean=8.3), \"${\\bf residual sugar}$\"(mean=2.5), \"${\\bf free sulfur dioxide}$\"(mean=15.6), \"${\\bf total sulfur dioxide}$\"(mean=45.9), \"${\\bf pH}$\"(mean=3.3), \"${\\bf alcohol}$\"(mean=10.4).The features, which are not scaled are: \"${\\bf volatile acidity}$\", \"${\\bf citric acid}$\", \"${\\bf chlorides}$\", \"${\\bf density}$\", \"${\\bf sulphates}$\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85631fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAININGS_DATA_LENGTH = 800\n",
    "wine_csv_path = \"WineQT.csv\"\n",
    "\n",
    "data_frame = pd.read_csv(wine_csv_path)\n",
    "\n",
    "# scale the input data\n",
    "features_to_scale = [\"fixed acidity\", \"residual sugar\", \"free sulfur dioxide\", \"total sulfur dioxide\", \"pH\", \"alcohol\"]\n",
    "scaler = StandardScaler()\n",
    "for feature in features_to_scale:\n",
    "    data_frame[feature] = scaler.fit_transform(data_frame[feature].to_frame())\n",
    "  \n",
    "\n",
    "data_frame = data_frame.drop(\"Id\", axis=1)\n",
    "X = data_frame.drop(\"quality\", axis=1)\n",
    "X = np.asarray(X)\n",
    "Y = data_frame[[\"quality\"]]\n",
    "Y = np.asarray(Y)\n",
    "\n",
    "# encod the output data\n",
    "encoder = OneHotEncoder()\n",
    "Y = encoder.fit_transform(Y).toarray()\n",
    "\n",
    "x_train, x_test = np.split(X, indices_or_sections=[TRAININGS_DATA_LENGTH])\n",
    "y_train, y_test = np.split(Y, indices_or_sections=[TRAININGS_DATA_LENGTH])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a089d363",
   "metadata": {},
   "source": [
    "## Model Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a700f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_builder:\n",
    "    \n",
    "    input_dim = x_train.shape[1]\n",
    "    output_dim = y_train.shape[1]\n",
    "    learning_rate = 0\n",
    "    batch_size = 0\n",
    "    saved_model_path = \"/Users/tombohlmann/Desktop/ml/wineTesting/safedModel\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.learning_rate = 0.01\n",
    "        self.batch_size = 50\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a neuronal Network with 3 hidden layers.\n",
    "    Input: layer_1_units: amount of nodes in the first hidden layer\n",
    "           layer_2_units: amount of nodes in the second hidden layer\n",
    "           layer_3_units: amount of nodes in the third hidden layer\n",
    "           act_fct: Activation function used in all 3 hidden layers\n",
    "           loss: loss Function used for the model\n",
    "           dr: Dropout rate in every hidden layer\n",
    "    Output: Dense neuronal Network (DNN) with 3 hidden layer. The optimizer is Adam \n",
    "                and the metrics for the network is the accuracy\n",
    "            The input-dimensions are self.inpt_dim and and the output-dimensions are self.output_dim.\n",
    "    \"\"\"\n",
    "    def build_model (self, layer_1_units=6, layer_2_units=16, layer_3_units=14, act_fct='sigmoid', loss='mse', dr=0.5):\n",
    "        \n",
    "        if self.saved_model_path != None:\n",
    "            print(\"---model loaded---\")\n",
    "            return load_model(self.saved_model_path)\n",
    "            \n",
    "        \n",
    "        \n",
    "        input_vec = Input(shape=self.input_dim, name='input')\n",
    "        \n",
    "        # 1\n",
    "        layer = Dense(layer_1_units, name=\"dense_1\") (input_vec)\n",
    "        layer = Dropout(dr, name=\"dr_1\") (layer)\n",
    "        layer = Activation(act_fct, name=\"activation_1\") (layer)\n",
    "        \n",
    "        # 2\n",
    "        layer = BatchNormalization() (layer)\n",
    "        layer = Dense(layer_2_units, name=\"dense_2\") (layer)\n",
    "        layer = Dropout(dr, name=\"dr_2\") (layer)\n",
    "        layer = Activation(act_fct, name=\"activation_2\") (layer)\n",
    "        \n",
    "        # 3\n",
    "        layer = BatchNormalization() (layer)\n",
    "        layer = Dense(layer_3_units, name=\"dense_3\") (layer)\n",
    "        layer = Dropout(dr, name=\"dr_3\") (layer)\n",
    "        layer = Activation(act_fct, name=\"activation_3\") (layer)\n",
    "        \n",
    "        output = Dense(self.output_dim, activation='sigmoid', name='output') (layer)\n",
    "        \n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        \n",
    "        model = Model(input_vec, output, name='classifier')\n",
    "        model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\n",
    "    \n",
    "        return model\n",
    "    \n",
    "    \"\"\"\n",
    "    To find optimal hyperparameters for the DNN the KerasClassifier evaluates different settings for the hyperparameters.\n",
    "    Output: Best hyperparameters (HP) for the DNN.\n",
    "    \n",
    "    \"\"\"\n",
    "    def find_hyperparameters(self):\n",
    "        kc = KerasClassifier(build_fn=self.build_model, epochs=20, batch_size=50, verbose=0)\n",
    "        grid_space = dict(layer_1_units=[6, 8, 10, 12, 14, 16],\n",
    "                         layer_2_units=[6, 8, 10, 12, 14, 16],\n",
    "                         layer_3_units=[6, 8, 10, 12, 14, 16],\n",
    "                         act_fct=['sigmoid', 'relu'])\n",
    "\n",
    "        gscv = GridSearchCV(estimator=kc, param_grid=grid_space, n_jobs=1, cv=2, verbose=2)\n",
    "        gscv_res = gscv.fit(x_train, y_train, validation_data=(x_test, y_test))\n",
    "        return gscv_res.best_params_\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Train the DNN for 200 epochs. The model is trained on the trainings data and tested on the test data.\n",
    "    The trainings data is divided into batches, which are shuffled before every trainings iteration. If no progress\n",
    "    in the metrics occurs, the learning rate is lowered or the trainings process is stopped early.\n",
    "    Input: verbose: 0 means, nothing is displayed during the trainings process. \n",
    "                    1 means, information about the iteration of a trianings process is displayed.\n",
    "    Output: -Trained model \n",
    "            - history of the trainings process\n",
    "    \"\"\"\n",
    "    def train_model(self, verbose=1):\n",
    "        epochs = 200\n",
    "        model = self.build_model()\n",
    "    \n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=10, factor=0.1, mode='min', verbose=verbose)\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True, verbose=verbose)\n",
    "        hst = model.fit(x_train, y_train, epochs=epochs, batch_size=self.batch_size, shuffle=True, callbacks=[reduce_lr, early_stop], validation_data=(x_test, y_test), verbose=verbose)\n",
    "        \n",
    "        \n",
    "        model.save(self.saved_model_path)\n",
    "        return model, hst\n",
    "    \n",
    "    \"\"\"\n",
    "    Input: beam_size: amount of different trained models.\n",
    "    Output: List of trained models, their trainings history and their accuracy, sorted by their accuracy.\n",
    "    \"\"\"\n",
    "    def beam_search(self, beam_size=10):\n",
    "        \n",
    "            models = []\n",
    "\n",
    "            for i in range(beam_size):\n",
    "                print(\"Model_\" + str(i) + \" start Trainig.\")\n",
    "\n",
    "                model, hst = self.train_model(verbose=0)\n",
    "                accuracy = model.evaluate(x_test, y_test, verbose=0)[1]\n",
    "                models.append((model, hst, accuracy))\n",
    "\n",
    "                print(\"Model_\" + str(i) + \" Training done. Accuracy=\" + str(accuracy))\n",
    "                print(\"------ Next ------\")\n",
    "\n",
    "            models.sort(key= lambda tupl : tupl[2], reverse=True)\n",
    "            print(\"!!!Models sorted by accuracy (increasingly)!!!\")\n",
    "            return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75958330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---model loaded---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-05 11:19:51.556992: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-05 11:19:51.557459: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "builder = Model_builder()\n",
    "\n",
    "model = builder.build_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
